{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import pprint\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import typing\n",
    "from typing import Tuple, List\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Our corpus uses the following start and end tokens.\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Vectors are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, translation, etc. It is therefore important to build some intuition as to their strengths and weaknesses. Here, we will explore two types of word vectors: those derived from co-occurrence matrices, and those derived via GloVe.\n",
    "\n",
    "The terms \"word vectors\" and \"word embeddings\" are often used interchangeably. The term \"embedding\" refers to the fact that we are encoding aspects of a word's meaning in a lower dimensional space. As Wikipedia states, \"conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension\".\n",
    "\n",
    "##### Part 1: Count-Based Word Vectors\n",
    "Most word vector models start from the following idea:\n",
    "\n",
    "*You shall know a word by the company it keeps (Firth, J. R. 1957:11)*\n",
    "\n",
    "Many word vector implementations are driven by the idea that similar words, i.e., (near) synonyms, will be used in similar contexts. As a result, similar words will often be spoken or written along with a shared subset of words, i.e., contexts. By examining these contexts, we can try to develop embeddings for our words. With this intuition in mind, many \"old school\" approaches to constructing word vectors relied on word counts. Here we elaborate upon one of those strategies, co-occurrence matrices.\n",
    "\n",
    "##### Co-Occurrence\n",
    "A co-occurrence matrix counts how often things co-occur in some environment. Given some word 𝑤𝑖 occurring in the document, we consider the context window surrounding 𝑤𝑖. Supposing our fixed window size is 𝑛, then this is the 𝑛 preceding and 𝑛 subsequent words in that document, i.e. words 𝑤𝑖−𝑛…𝑤𝑖−1 and 𝑤𝑖+1…𝑤𝑖+𝑛. We build a co-occurrence matrix 𝑀, which is a symmetric word-by-word matrix in which 𝑀𝑖𝑗 is the number of times 𝑤𝑗 appears inside 𝑤𝑖's window among all documents.\n",
    "\n",
    "The rows (or columns) of a co-occurrence matrix provide one type of word vectors (those based on word-word co-occurrence), but the vectors will be large in general (linear in the number of distinct words in a corpus). Thus, our next step is to run *dimensionality reduction*. In particular, we will run *SVD (Singular Value Decomposition)*, which is a kind of generalized *PCA (Principal Components Analysis)* to select the top 𝑘 *principal components*. Here's a visualization of dimensionality reduction with SVD. In this picture our co-occurrence matrix is 𝐴 with 𝑛 rows corresponding to 𝑛 words. We obtain a full matrix decomposition, with the singular values ordered in the diagonal 𝑆 matrix, and our new, shorter length-𝑘 word vectors in 𝑈𝑘.\n",
    "\n",
    "![Picture of an SVD](./imgs/svd.png \"SVD\")\n",
    "\n",
    "This reduced-dimensionality co-occurrence representation preserves semantic relationships between words, e.g. *doctor* and *hospital* will be closer than *doctor* and *dog*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(category: str) -> List[List[str]]:\n",
    "    \"\"\" Read files from the specified Reuter's category.\n",
    "        Params:\n",
    "            category (string): category name\n",
    "        Return:\n",
    "            list of lists, with words from each of the processed files\n",
    "    \"\"\"\n",
    "    files = reuters.fileids(category)\n",
    "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]\n",
    "\n",
    "\n",
    "def distinct_words(corpus: List[List[str]]) -> Tuple[List[str], int]:\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            corpus_words (list of strings): list of distinct words across the corpus, sorted\n",
    "            num_corpus_words (integer): number of distinct words across the corpus\n",
    "    \"\"\"\n",
    "    # Flatten list of lists.\n",
    "    flattened_list = [word for document in corpus for word in document]\n",
    "    \n",
    "    # Remove duplicates.\n",
    "    corpus_words = set(flattened_list)\n",
    "    \n",
    "    num_corpus_words = len(corpus_words)\n",
    "\n",
    "    return sorted(list(corpus_words)), num_corpus_words\n",
    "\n",
    "\n",
    "def compute_co_occurrence_matrix(corpus: List[List[str]], window_size: int = 4) -> Tuple[np.matrix, dict]:\n",
    "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size.    \n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "            window_size (int): size of context window\n",
    "        Return:\n",
    "            M (a symmetric numpy matrix of shape (number of unique words i n the corpus , number of unique words in the corpus)): \n",
    "                Co-occurence matrix of word counts. \n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
    "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
    "    \"\"\"\n",
    "    words, num_words = distinct_words(corpus)\n",
    "\n",
    "    word2Ind = dict()    \n",
    "    for idx, word in enumerate(words):\n",
    "        word2Ind[word] = idx\n",
    "\n",
    "    M = np.zeros((num_words, num_words))\n",
    "    for document in corpus:\n",
    "        for word_idx_in_doc, word in enumerate(document):\n",
    "            indices_in_doc = list(range(word_idx_in_doc - window_size, word_idx_in_doc + window_size + 1))\n",
    "            indices_in_doc = [x for x in indices_in_doc if x >= 0 and x <= (len(document)-1) and x != word_idx_in_doc]\n",
    "            for i in indices_in_doc:\n",
    "                matrix_idx_this_word = word2Ind[word]\n",
    "                matrix_idx_neighbor = word2Ind[document[i]]\n",
    "                M[(matrix_idx_this_word, matrix_idx_neighbor)] += 1\n",
    "\n",
    "    return M, word2Ind\n",
    "\n",
    "\n",
    "# Note: All of numpy, scipy, and scikit-learn (sklearn) provide some implementation of SVD, but\n",
    "# only scipy and sklearn provide an implementation of Truncated SVD, and only sklearn provides\n",
    "# an efficient randomized algorithm for calculating large-scale Truncated SVD. So please use\n",
    "# sklearn.decomposition.TruncatedSVD.\n",
    "def reduce_to_k_dim(M: np.matrix, k: int = 2) -> np.matrix:\n",
    "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    \n",
    "        Params:\n",
    "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurence matrix of word counts\n",
    "            k (int): embedding size of each word after dimension reduction\n",
    "        Return:\n",
    "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
    "                    In terms of the SVD from math class, this actually returns U * S\n",
    "    \"\"\"    \n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "    \n",
    "    # X = sparse_random(100, 100, density=0.01, format='csr', random_state=SEED)\n",
    "    svd = TruncatedSVD(n_components=k, n_iter=n_iters, random_state=SEED)\n",
    "    M_reduced = svd.fit_transform(M)\n",
    "    \n",
    "    print(\"Done.\")\n",
    "    return M_reduced\n",
    "\n",
    "\n",
    "def plot_embeddings(M_reduced: np.matrix, word2Ind: dict, words: List[str]) -> None:\n",
    "    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
    "        NOTE: do not plot all the words listed in M_reduced / word2Ind.\n",
    "        Include a label next to each point.\n",
    "        \n",
    "        Params:\n",
    "            M_reduced (numpy matrix of shape (number of unique words in the corpus, 2)): matrix of 2-dimensioal word embeddings\n",
    "            word2Ind (dict): dictionary that maps word to indices for matrix M\n",
    "            words (list of strings): words whose embeddings we want to visualize\n",
    "    \"\"\"\n",
    "    for i, word in enumerate(words):\n",
    "        idx = word2Ind[word]\n",
    "        x, y = M_reduced[idx]\n",
    "        plt.scatter(x, y, marker='x', color='red')\n",
    "        plt.text(x, y, word, fontsize=9)\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_corpus = read_corpus(category=\"crude\")\n",
    "# pprint.pprint(reuters_corpus[:3], compact=True, width=100)\n",
    "\n",
    "corpus_words, num_corpus_words = distinct_words(reuters_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Passed All Tests!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Sanity check to ensure the correctness of distinct_words()\n",
    "# ---------------------\n",
    "\n",
    "# Define toy corpus\n",
    "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
    "test_corpus_words, num_corpus_words = distinct_words(test_corpus)\n",
    "\n",
    "# Correct answers\n",
    "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
    "ans_num_corpus_words = len(ans_test_corpus_words)\n",
    "\n",
    "# Test correct number of words\n",
    "assert(num_corpus_words == ans_num_corpus_words), \"Incorrect number of distinct words. Correct: {}. Yours: {}\".format(ans_num_corpus_words, num_corpus_words)\n",
    "\n",
    "# Test correct words\n",
    "assert (test_corpus_words == ans_test_corpus_words), \"Incorrect corpus_words.\\nCorrect: {}\\nYours:   {}\".format(str(ans_test_corpus_words), str(test_corpus_words))\n",
    "\n",
    "# Print Success\n",
    "print (\"-\" * 80)\n",
    "print(\"Passed All Tests!\")\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Passed All Tests!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Sanity check to ensure the correctness of compute_co_occurrence_matrix()\n",
    "# ---------------------\n",
    "\n",
    "# Define toy corpus and get student's co-occurrence matrix\n",
    "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
    "M_test, word2Ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
    "\n",
    "# Correct M and word2Ind\n",
    "M_test_ans = np.array( \n",
    "    [[0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,],\n",
    "     [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,],\n",
    "     [0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,],\n",
    "     [0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,],\n",
    "     [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,],\n",
    "     [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,],\n",
    "     [1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,],\n",
    "     [0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,],\n",
    "     [0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,],\n",
    "     [1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,]]\n",
    ")\n",
    "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
    "word2Ind_ans = dict(zip(ans_test_corpus_words, range(len(ans_test_corpus_words))))\n",
    "\n",
    "# Test correct word2Ind\n",
    "assert (word2Ind_ans == word2Ind_test), \"Your word2Ind is incorrect:\\nCorrect: {}\\nYours: {}\".format(word2Ind_ans, word2Ind_test)\n",
    "\n",
    "# Test correct M shape\n",
    "assert (M_test.shape == M_test_ans.shape), \"M matrix has incorrect shape.\\nCorrect: {}\\nYours: {}\".format(M_test.shape, M_test_ans.shape)\n",
    "\n",
    "# Test correct M values\n",
    "for w1 in word2Ind_ans.keys():\n",
    "    idx1 = word2Ind_ans[w1]\n",
    "    for w2 in word2Ind_ans.keys():\n",
    "        idx2 = word2Ind_ans[w2]\n",
    "        student = M_test[idx1, idx2]\n",
    "        correct = M_test_ans[idx1, idx2]\n",
    "        if student != correct:\n",
    "            print(\"Correct M:\")\n",
    "            print(M_test_ans)\n",
    "            print(\"Your M: \")\n",
    "            print(M_test)\n",
    "            raise AssertionError(\"Incorrect count at index ({}, {})=({}, {}) in matrix M. Yours has {} but should have {}.\".format(idx1, idx2, w1, w2, student, correct))\n",
    "\n",
    "# Print Success\n",
    "print (\"-\" * 80)\n",
    "print(\"Passed All Tests!\")\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Truncated SVD over 10 words...\n",
      "Done.\n",
      "--------------------------------------------------------------------------------\n",
      "Passed All Tests!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Sanity check to ensure that  M_reduced has the right dimensions.\n",
    "# ---------------------\n",
    "\n",
    "# Define toy corpus and run student code\n",
    "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
    "M_test, word2Ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
    "M_test_reduced = reduce_to_k_dim(M_test, k=2)\n",
    "\n",
    "# Test proper dimensions\n",
    "assert (M_test_reduced.shape[0] == 10), \"M_reduced has {} rows; should have {}\".format(M_test_reduced.shape[0], 10)\n",
    "assert (M_test_reduced.shape[1] == 2), \"M_reduced has {} columns; should have {}\".format(M_test_reduced.shape[1], 2)\n",
    "\n",
    "# Print Success\n",
    "print (\"-\" * 80)\n",
    "print(\"Passed All Tests!\")\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Outputted Plot:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Sanity check. The plot produced should look like the \"test solution plot\" depicted below. \n",
    "# ---------------------\n",
    "\n",
    "print (\"-\" * 80)\n",
    "print (\"Outputted Plot:\")\n",
    "\n",
    "M_reduced_plot_test = np.array([[1, 1], [-1, -1], [1, -1], [-1, 1], [0, 0]])\n",
    "word2Ind_plot_test = {'test1': 0, 'test2': 1, 'test3': 2, 'test4': 3, 'test5': 4}\n",
    "words = ['test1', 'test2', 'test3', 'test4', 'test5']\n",
    "plot_embeddings(M_reduced_plot_test, word2Ind_plot_test, words)\n",
    "\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**Test Plot Solution**</font>\n",
    "<br>\n",
    "<img src=\"./imgs/test_plot.png\" width=40% style=\"float: left;\"> </img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.5: Co-Occurrence Plot Analysis\n",
    "\n",
    "Put together all the parts we have written. Compute the co-occurrence matrix with fixed window of 4 (the default window size), over the Reuters \"crude\" corpus. Then, use TruncatedSVD to compute 2-dimensional embeddings of each word. TruncatedSVD returns U\\*S, so we need to normalize the returned vectors, so that all the vectors will appear around the unit circle (therefore closeness is directional closeness). \n",
    "**Note**: The line of code below that does the normalizing uses the NumPy concept of *broadcasting*. If you don't know about broadcasting, check out\n",
    "[Computation on Arrays: Broadcasting by Jake VanderPlas](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html).\n",
    "\n",
    "What clusters together in 2-dimensional embedding space? What doesn't cluster together that you might think should have?  **Note:** \"bpd\" stands for \"barrels per day\" and is a commonly used abbreviation in crude oil topic articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Truncated SVD over 8185 words...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Produce plot on Reuters data.\n",
    "# ------------------------------\n",
    "reuters_corpus = read_corpus(category='crude')\n",
    "M_co_occurrence, word2Ind_co_occurrence = compute_co_occurrence_matrix(reuters_corpus)\n",
    "M_reduced_co_occurrence = reduce_to_k_dim(M_co_occurrence, k=2)\n",
    "\n",
    "# Rescale (normalize) the rows to make them each of unit-length.\n",
    "M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\n",
    "M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis] # broadcasting\n",
    "\n",
    "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']\n",
    "\n",
    "plot_embeddings(M_normalized, word2Ind_co_occurrence, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that countries are lumped together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Prediction-Based Word Vectors (15 points)\n",
    "\n",
    "More recently, prediction-based word vectors have demonstrated better performance, such as word2vec and GloVe (which also utilizes the benefit of counts). Here, we shall explore the embeddings produced by GloVe [GloVe's original paper](https://nlp.stanford.edu/pubs/glove.pdf).\n",
    "\n",
    "If this is your first time running these cells, i.e. download the embedding model, it will take about 15 minutes to run. If you've run these cells before, rerunning them will load the model without redownloading it, which will take about 1 to 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model() -> KeyedVectors:\n",
    "    \"\"\" Load GloVe Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All 400000 embeddings, each lengh 200\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
    "    print(\"Loaded vocab size %i\" % len(wv_from_bin.vocab.keys()))\n",
    "    return wv_from_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size 400000\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Run Cell to Load Word Vectors\n",
    "# Note: This will take several minutes\n",
    "# -----------------------------------\n",
    "wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing dimensionality of Word Embeddings\n",
    "Let's directly compare the GloVe embeddings to those of the co-occurrence matrix. In order to avoid running out of memory, we will work with a sample of 10000 GloVe vectors instead.\n",
    "1. Put 10000 Glove vectors into a matrix M\n",
    "2. Run reduce_to_k_dim (your Truncated SVD function) to reduce the vectors from 200-dimensional to 2-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_of_vectors(wv_from_bin: KeyedVectors, required_words: list = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']) -> Tuple[np.matrix, dict]:\n",
    "    \"\"\" Put the GloVe vectors into a matrix M.\n",
    "        Param:\n",
    "            wv_from_bin: KeyedVectors object; the 400000 GloVe vectors loaded from file\n",
    "        Return:\n",
    "            M: numpy matrix shape (num words, 200) containing the vectors\n",
    "            word2Ind: dictionary mapping each word to its row number in M\n",
    "    \"\"\"\n",
    "    import random\n",
    "    words = list(wv_from_bin.vocab.keys())\n",
    "    print(\"Shuffling words ...\")\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(words)\n",
    "    words = words[:10000]\n",
    "    print(\"Putting %i words into word2Ind and matrix M...\" % len(words))\n",
    "    word2Ind = {}\n",
    "    M = []\n",
    "    curInd = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(wv_from_bin.word_vec(w))\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    for w in required_words:\n",
    "        if w in words:\n",
    "            continue\n",
    "        try:\n",
    "            M.append(wv_from_bin.word_vec(w))\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    M = np.stack(M)\n",
    "    print(\"Done.\")\n",
    "    return M, word2Ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling words ...\n",
      "Putting 10000 words into word2Ind and matrix M...\n",
      "Done.\n",
      "Running Truncated SVD over 10010 words...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Run Cell to Reduce 200-Dimensional Word Embeddings to k Dimensions\n",
    "# Note: This should be quick to run\n",
    "# -----------------------------------------------------------------\n",
    "M, word2Ind = get_matrix_of_vectors(wv_from_bin)\n",
    "M_reduced = reduce_to_k_dim(M, k=2)\n",
    "\n",
    "# Rescale (normalize) the rows to make them each of unit-length\n",
    "M_lengths = np.linalg.norm(M_reduced, axis=1)\n",
    "M_reduced_normalized = M_reduced / M_lengths[:, np.newaxis] # broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to plot the 2D GloVe embeddings for `['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']`.\n",
    "\n",
    "What clusters together in 2-dimensional embedding space? What doesn't cluster together that you might think should have? How is the plot different from the one generated earlier from the co-occurrence matrix? What is a possible reason for causing the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']\n",
    "plot_embeddings(M_reduced_normalized, word2Ind, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countries are not grouped as tightly together as previously. Petroleum and oil are closer to one another, which makes sense, as they essentially convey the same meaning. One also finds energy and industry being embedded more closely together, which one can find reasonable as both terms refer to the meta-level (the energy industry)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "Now that we have word vectors, we need a way to quantify the similarity between individual words, according to these vectors. One such metric is cosine-similarity. We will be using this to find words that are \"close\" and \"far\" from one another.\n",
    "\n",
    "We can think of n-dimensional vectors as points in n-dimensional space. If we take this perspective [L1](http://mathworld.wolfram.com/L1-Norm.html) and [L2](http://mathworld.wolfram.com/L2-Norm.html) Distances help quantify the amount of space \"we must travel\" to get between these two points. Another approach is to examine the angle between two vectors. From trigonometry we know that:\n",
    "\n",
    "<img src=\"./imgs/inner_product.png\" width=20% style=\"float: center;\"></img>\n",
    "\n",
    "Instead of computing the actual angle, we can leave the similarity in terms of $similarity = cos(\\Theta)$. Formally the [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) $s$ between two vectors $p$ and $q$ is defined as:\n",
    "\n",
    "$$s = \\frac{p \\cdot q}{||p|| ||q||}, \\textrm{ where } s \\in [-1, 1] $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2: Words with Multiple Meanings\n",
    "Polysemes and homonyms are words that have more than one meaning ([wiki page](https://en.wikipedia.org/wiki/Polysemy)). Find a word with at least 2 different meanings such that the top-10 most similar words (according to cosine similarity) contain related words from *both* meanings. For example, \"leaves\" has both \"vanishes\" and \"stalks\" in the top 10, and \"scoop\" has both \"handed_waffle_cone\" and \"lowdown\". You will probably need to try several polysemous or homonymic words before you find one. Please state the word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous or homonymic words you tried didn't work (i.e. the top-10 most similar words only contain **one** of the meanings of the words)?\n",
    "\n",
    "**Note**: You should use the `wv_from_bin.most_similar(word)` function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance please check the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "board\n",
      "('boards', 0.6880484223365784)\n",
      "('directors', 0.6878399848937988)\n",
      "('commission', 0.6532573103904724)\n",
      "('trustees', 0.6526336669921875)\n",
      "('executive', 0.6468799710273743)\n",
      "('commissioners', 0.6252285242080688)\n",
      "('council', 0.6131470203399658)\n",
      "('committee', 0.6117008924484253)\n",
      "('chairman', 0.5796202421188354)\n",
      "('panel', 0.5716795325279236)\n",
      "\n",
      "bank\n",
      "('banks', 0.7625691890716553)\n",
      "('banking', 0.6818838119506836)\n",
      "('central', 0.6283639669418335)\n",
      "('financial', 0.6166563034057617)\n",
      "('credit', 0.6049750447273254)\n",
      "('lending', 0.5980608463287354)\n",
      "('monetary', 0.5963002443313599)\n",
      "('bankers', 0.5913101434707642)\n",
      "('loans', 0.5802938938140869)\n",
      "('investment', 0.5740202069282532)\n",
      "\n",
      "leaves\n",
      "('ends', 0.6128067970275879)\n",
      "('leaf', 0.6027014851570129)\n",
      "('stems', 0.5998532176017761)\n",
      "('takes', 0.5902854800224304)\n",
      "('leaving', 0.5761634111404419)\n",
      "('grows', 0.5663397312164307)\n",
      "('flowers', 0.5600922107696533)\n",
      "('turns', 0.5536050796508789)\n",
      "('leave', 0.5496848821640015)\n",
      "('goes', 0.5434924960136414)\n",
      "\n",
      "bear\n",
      "('bears', 0.6849422454833984)\n",
      "('grizzly', 0.6034084558486938)\n",
      "('wolf', 0.5907024145126343)\n",
      "('stearns', 0.5704878568649292)\n",
      "('lion', 0.5357851982116699)\n",
      "('bearing', 0.5106834769248962)\n",
      "('dog', 0.5077008605003357)\n",
      "('big', 0.49132293462753296)\n",
      "('bore', 0.4897792339324951)\n",
      "('deer', 0.4877018928527832)\n",
      "\n",
      "right\n",
      "('left', 0.716508150100708)\n",
      "('if', 0.6925000548362732)\n",
      "(\"n't\", 0.6774846315383911)\n",
      "('back', 0.6770385503768921)\n",
      "('just', 0.6740819811820984)\n",
      "('but', 0.667771577835083)\n",
      "('out', 0.6671877503395081)\n",
      "('put', 0.6658940315246582)\n",
      "('hand', 0.6634082794189453)\n",
      "('want', 0.6615420579910278)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_top_10_similar_words(input_word):\n",
    "    wv_from_bin.most_similar(input_word)\n",
    "\n",
    "    return top_10_similar_words\n",
    "\n",
    "words = ['board', 'bank', 'leaves', 'bear', 'right']\n",
    "for word in words:\n",
    "    print(word, *wv_from_bin.most_similar(word), sep='\\n')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two good examples of homonyms in our corpus include:\n",
    "- leaves (with the meaning of tree leaves and leaving respectively);\n",
    "- right (with the meaning of correct and the direction respectively).\n",
    "\n",
    "Three homonyms present in our corpus which do *not* show multiple meanings in the top-10 words by cosine similarity includes:\n",
    "- board (only the \"board of directors\" meaning shows up, not the \"plank\" kind of board);\n",
    "- bank (only the financial institution shows up, not the river bank meaning of the word).\n",
    "\n",
    "A semi-good example includes:\n",
    "- bear (mainly in the meaning of the animal, although we also get \"bearing\").\n",
    "\n",
    "The reason for our results can be traced back to the nature of our corpus. It is collected from Reuters. Therefore, it is biased towards newsworthy words, i.e. financial institutions over river banks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3: Synonyms & Antonyms (2 points) [code + written] \n",
    "\n",
    "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
    "\n",
    "Find three words (w1,w2,w3) where w1 and w2 are synonyms and w1 and w3 are antonyms, but Cosine Distance(w1,w3) < Cosine Distance(w1,w2). For example, w1=\"happy\" is closer to w3=\"sad\" than to w2=\"cheerful\". \n",
    "\n",
    "Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
    "\n",
    "You should use the the `wv_from_bin.distance(w1, w2)` function here in order to compute the cosine distance between two words. Please see the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.distance)__ for further assistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms\n",
      "('begin', 'commence') \t 0.42311644554138184\n",
      "('begin', 'start') \t 0.19364678859710693\n",
      "('begin', 'initiate') \t 0.6234182119369507\n",
      "antonyms\n",
      "('begin', 'terminate') \t 0.7202378213405609\n",
      "('begin', 'finish') \t 0.5839853882789612\n",
      "('begin', 'stop') \t 0.42383432388305664\n",
      "('begin', 'halt') \t 0.5599273443222046\n",
      "('begin', 'conclude') \t 0.4550349712371826\n",
      "('begin', 'cease') \t 0.5645579099655151\n"
     ]
    }
   ],
   "source": [
    "word = 'begin'\n",
    "synonyms = ['commence', 'start', 'initiate']\n",
    "antonyms = ['terminate', 'finish', 'stop', 'halt', 'conclude', 'cease']\n",
    "\n",
    "print('synonyms')\n",
    "for synonym in synonyms:\n",
    "    print(f'{word, synonym} \\t {wv_from_bin.distance(word, synonym)}')\n",
    "\n",
    "print('antonyms')\n",
    "for antonym in antonyms:\n",
    "    print(f'{word, antonym} \\t {wv_from_bin.distance(word, antonym)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 'begin' and 'initiate' have a relatively large Cosine Distance of 0.623 even though they can be considered synonyms.\n",
    "\n",
    "More interestingly, we see also that 'begin' and 'stop' display a Cosine Distance of 0.424, which is significantly smaller than the Cosine Distance between 'begin' and 'initiate', even though 'begin' and 'stop' are direct antonyms.\n",
    "\n",
    "One explanation to the counter-intuitive result is the possibility that 'begin' and 'stop' show up together frequently. Since they would often be found in the same context, they would be mapped to a similar point in our multi-dimensional word-embedding space.\n",
    "\n",
    "For example, antonyms could be used in the same sentence to compare two things. For this reason, GloVe could result in these antonyms having a relatively small cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
