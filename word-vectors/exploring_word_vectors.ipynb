{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import pprint\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import typing\n",
    "from typing import Tuple, List\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Our corpus uses the following start and end tokens.\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Vectors are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, translation, etc. It is therefore important to build some intuition as to their strengths and weaknesses. Here, we will explore two types of word vectors: those derived from co-occurrence matrices, and those derived via GloVe.\n",
    "\n",
    "The terms \"word vectors\" and \"word embeddings\" are often used interchangeably. The term \"embedding\" refers to the fact that we are encoding aspects of a word's meaning in a lower dimensional space. As Wikipedia states, \"conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension\".\n",
    "\n",
    "##### Part 1: Count-Based Word Vectors\n",
    "Most word vector models start from the following idea:\n",
    "\n",
    "*You shall know a word by the company it keeps (Firth, J. R. 1957:11)*\n",
    "\n",
    "Many word vector implementations are driven by the idea that similar words, i.e., (near) synonyms, will be used in similar contexts. As a result, similar words will often be spoken or written along with a shared subset of words, i.e., contexts. By examining these contexts, we can try to develop embeddings for our words. With this intuition in mind, many \"old school\" approaches to constructing word vectors relied on word counts. Here we elaborate upon one of those strategies, co-occurrence matrices.\n",
    "\n",
    "##### Co-Occurrence\n",
    "A co-occurrence matrix counts how often things co-occur in some environment. Given some word ð‘¤ð‘– occurring in the document, we consider the context window surrounding ð‘¤ð‘–. Supposing our fixed window size is ð‘›, then this is the ð‘› preceding and ð‘› subsequent words in that document, i.e. words ð‘¤ð‘–âˆ’ð‘›â€¦ð‘¤ð‘–âˆ’1 and ð‘¤ð‘–+1â€¦ð‘¤ð‘–+ð‘›. We build a co-occurrence matrix ð‘€, which is a symmetric word-by-word matrix in which ð‘€ð‘–ð‘— is the number of times ð‘¤ð‘— appears inside ð‘¤ð‘–'s window among all documents.\n",
    "\n",
    "The rows (or columns) of a co-occurrence matrix provide one type of word vectors (those based on word-word co-occurrence), but the vectors will be large in general (linear in the number of distinct words in a corpus). Thus, our next step is to run *dimensionality reduction*. In particular, we will run *SVD (Singular Value Decomposition)*, which is a kind of generalized *PCA (Principal Components Analysis)* to select the top ð‘˜ *principal components*. Here's a visualization of dimensionality reduction with SVD. In this picture our co-occurrence matrix is ð´ with ð‘› rows corresponding to ð‘› words. We obtain a full matrix decomposition, with the singular values ordered in the diagonal ð‘† matrix, and our new, shorter length-ð‘˜ word vectors in ð‘ˆð‘˜.\n",
    "\n",
    "![Picture of an SVD](./imgs/svd.png \"SVD\")\n",
    "\n",
    "This reduced-dimensionality co-occurrence representation preserves semantic relationships between words, e.g. *doctor* and *hospital* will be closer than *doctor* and *dog*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(category: str) -> List[List[str]]:\n",
    "    \"\"\" Read files from the specified Reuter's category.\n",
    "        Params:\n",
    "            category (string): category name\n",
    "        Return:\n",
    "            list of lists, with words from each of the processed files\n",
    "    \"\"\"\n",
    "    files = reuters.fileids(category)\n",
    "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]\n",
    "\n",
    "\n",
    "def distinct_words(corpus: List[List[str]]) -> Tuple[List[str], int]:\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            corpus_words (list of strings): list of distinct words across the corpus, sorted\n",
    "            num_corpus_words (integer): number of distinct words across the corpus\n",
    "    \"\"\"\n",
    "    # Flatten list of lists.\n",
    "    flattened_list = [word for document in corpus for word in document]\n",
    "    \n",
    "    # Remove duplicates.\n",
    "    corpus_words = set(flattened_list)\n",
    "    \n",
    "    num_corpus_words = len(corpus_words)\n",
    "\n",
    "    return sorted(list(corpus_words)), num_corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_corpus = read_corpus(category=\"crude\")\n",
    "# pprint.pprint(reuters_corpus[:3], compact=True, width=100)\n",
    "\n",
    "corpus_words, num_corpus_words = distinct_words(reuters_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Passed All Tests!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Sanity check to ensure the correctness of distinct_words()\n",
    "# ---------------------\n",
    "\n",
    "# Define toy corpus\n",
    "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
    "test_corpus_words, num_corpus_words = distinct_words(test_corpus)\n",
    "\n",
    "# Correct answers\n",
    "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
    "ans_num_corpus_words = len(ans_test_corpus_words)\n",
    "\n",
    "# Test correct number of words\n",
    "assert(num_corpus_words == ans_num_corpus_words), \"Incorrect number of distinct words. Correct: {}. Yours: {}\".format(ans_num_corpus_words, num_corpus_words)\n",
    "\n",
    "# Test correct words\n",
    "assert (test_corpus_words == ans_test_corpus_words), \"Incorrect corpus_words.\\nCorrect: {}\\nYours:   {}\".format(str(ans_test_corpus_words), str(test_corpus_words))\n",
    "\n",
    "# Print Success\n",
    "print (\"-\" * 80)\n",
    "print(\"Passed All Tests!\")\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
